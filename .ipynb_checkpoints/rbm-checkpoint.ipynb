{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnames = ['movie_id', 'title', 'genre']\n",
    "movies_df = pd.read_table('ml-1m/movies.dat', names = mnames, sep = \"::\", engine = 'python', encoding='ISO-8859-1')\n",
    "# Loading the cleaned datasets\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings_df = pd.read_table(\"ml-1m/ratings.dat\", header =None, sep='::',names=rnames, engine= 'python')\n",
    "uname = ['user_id','gender','age','occupation','zip']\n",
    "users_df = pd.read_table(\"ml-1m/users.dat\", sep='::', header = None, names=uname, engine='python')\n",
    "ratings_df.drop(columns=['timestamp'], inplace=True, axis=1)  # Remove useless features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the ratings dataset into the feature set (X) and target labels (y)\n",
    "X = ratings_df.drop(columns='rating')\n",
    "y = ratings_df[\"rating\"].values  # The movie ratings are the target variables we want to predict\n",
    "\n",
    "# Preparing train, validation and test datasets.\n",
    "# I have chosen a split ratio of 80%, 10%, 10%, because I want a somewhat large training set at the cost of a\n",
    "# smaller validation and test set. I do not think that a smaller validation (or test) dataset will negatively\n",
    "# impact the generalization ability of the chosen models, because I am only using rather simple ML models\n",
    "# with few hyperparamaters.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# Creating a complete training dataset with X_train and y_train\n",
    "train_df = X_train.copy()\n",
    "train_df[\"rating\"] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[\"rating\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_df)\n",
    "training_set = training_set.astype(\"int\")\n",
    "test_set = np.array(train_df)\n",
    "test_set = test_set.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1035,  594,  938,  914, 1836, 1029,  595,  260,  919, 1961, 1207,\n",
       "        745, 2028, 1270, 1962, 3186,  720, 2692, 1197, 2804, 2321, 2762,\n",
       "        661, 2797,  150, 3105, 2791, 1193, 1907, 2918, 1287, 1022,  783,\n",
       "       1246, 1545, 2355,  608,  531, 2018, 1566, 1721,  588, 3114])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[:,1][training_set[:,0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3952\n"
     ]
    }
   ],
   "source": [
    "#take max users id in train and test data\n",
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies =  int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
    "print(nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        ##id of movies that is rated by current users\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        \n",
    "        ##rate of movies that is given by current user\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        \n",
    "        #inialize ratings for all movies\n",
    "        #set 0 for movies that are not rated by current users\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        #movie id starts from 1, 1st movie will be 1st element in rating with index as 0\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "training_set = torch.FloatTensor(convert(training_set)).to(device)\n",
    "test_set = torch.FloatTensor(convert(test_set)).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM():\n",
    "\n",
    "    def __init__(self, num_visible, num_hidden, k, learning_rate=1e-3, momentum_coefficient=0.5, weight_decay=1e-4, use_cuda=True):\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_coefficient = momentum_coefficient\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        self.weights = torch.randn(num_visible, num_hidden, device=device) * 0.1\n",
    "        self.visible_bias = torch.ones(num_visible, device=device) * 0.5\n",
    "        self.hidden_bias = torch.zeros(num_hidden, device=device)\n",
    "\n",
    "        self.weights_momentum = torch.zeros(num_visible, num_hidden, device=device)\n",
    "        self.visible_bias_momentum = torch.zeros(num_visible, device=device)\n",
    "        self.hidden_bias_momentum = torch.zeros(num_hidden, device=device)\n",
    "\n",
    "    def sample_hidden(self, visible_probabilities):\n",
    "        hidden_activations = torch.matmul(visible_probabilities, self.weights) + self.hidden_bias\n",
    "        hidden_probabilities = self._sigmoid(hidden_activations)\n",
    "        return hidden_probabilities\n",
    "\n",
    "    def sample_visible(self, hidden_probabilities):\n",
    "        visible_activations = torch.matmul(hidden_probabilities, self.weights.t()) + self.visible_bias\n",
    "        visible_probabilities = self._sigmoid(visible_activations)\n",
    "        return visible_probabilities\n",
    "\n",
    "    def contrastive_divergence(self, input_data):\n",
    "        # Positive phase\n",
    "        positive_hidden_probabilities = self.sample_hidden(input_data)\n",
    "        positive_hidden_activations = (positive_hidden_probabilities >= self._random_probabilities(self.num_hidden)).float()\n",
    "        positive_associations = torch.matmul(input_data.t(), positive_hidden_activations)\n",
    "\n",
    "        # Negative phase\n",
    "        hidden_activations = positive_hidden_activations\n",
    "\n",
    "        for step in range(self.k):\n",
    "            visible_probabilities = self.sample_visible(hidden_activations)\n",
    "            visible_probabilities[input_data == 0] = 0\n",
    "            hidden_probabilities = self.sample_hidden(visible_probabilities)\n",
    "            hidden_activations = (hidden_probabilities >= self._random_probabilities(self.num_hidden)).float()\n",
    "\n",
    "        negative_visible_probabilities = visible_probabilities\n",
    "        negative_hidden_probabilities = hidden_probabilities\n",
    "\n",
    "        negative_associations = torch.matmul(negative_visible_probabilities.t(), negative_hidden_probabilities)\n",
    "\n",
    "        # Update parameters\n",
    "        self.weights_momentum *= self.momentum_coefficient\n",
    "        self.weights_momentum += (positive_associations - negative_associations)\n",
    "\n",
    "        self.visible_bias_momentum *= self.momentum_coefficient\n",
    "        self.visible_bias_momentum += torch.sum(input_data - negative_visible_probabilities, dim=0)\n",
    "\n",
    "        self.hidden_bias_momentum *= self.momentum_coefficient\n",
    "        self.hidden_bias_momentum += torch.sum(positive_hidden_probabilities - negative_hidden_probabilities, dim=0)\n",
    "\n",
    "        batch_size = input_data.size(0)\n",
    "        self.weights += self.weights_momentum * self.learning_rate / batch_size\n",
    "        self.visible_bias += self.visible_bias_momentum * self.learning_rate / batch_size\n",
    "        self.hidden_bias += self.hidden_bias_momentum * self.learning_rate / batch_size\n",
    "\n",
    "        self.weights -= self.weights * self.weight_decay  # L2 weight decay\n",
    "\n",
    "        # Compute reconstruction error\n",
    "        target = torch.flatten(input_data)[torch.flatten(input_data) > 0]\n",
    "        predict = torch.flatten(negative_visible_probabilities)[torch.flatten(input_data) > 0]\n",
    "        error = torch.sum((target - predict) ** 2)\n",
    "        return error\n",
    " \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def _random_probabilities(self, num):\n",
    "        random_probabilities = torch.rand(num, device=device)\n",
    "        return random_probabilities\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.weights, \"./rbm/weights.pt\")\n",
    "        torch.save(self.visible_bias, \"./rbm/visible_bias.pt\")\n",
    "        torch.save(self.hidden_bias, \"./rbm/hidden_bias.pt\")\n",
    "    \n",
    "    def load(self):\n",
    "        self.weights = torch.load(\"./rbm/weights.pt\").to(device)\n",
    "        self.visible_bias = torch.load(\"./rbm/visible_bias.pt\").to(device)\n",
    "        self.hidden_bias = torch.load(\"./rbm/hidden_bias.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info> Creating the architecture of the Neural Network\n",
      "<Info> epoch: 1 loss: 0.2754671653273555\n",
      "<Info> epoch: 2 loss: 0.2504577469387074\n",
      "<Info> epoch: 3 loss: 0.23913147373878435\n",
      "<Info> epoch: 4 loss: 0.23221015904325643\n",
      "<Info> epoch: 5 loss: 0.22751849003731295\n",
      "<Info> epoch: 6 loss: 0.22424095455902318\n",
      "<Info> epoch: 7 loss: 0.22195287499687297\n",
      "<Info> epoch: 8 loss: 0.22089200672398307\n",
      "<Info> epoch: 9 loss: 0.21896714264170997\n",
      "<Info> epoch: 10 loss: 0.2185055610185856\n",
      "<Info> epoch: 11 loss: 0.21718566845732215\n",
      "<Info> epoch: 12 loss: 0.21718012810964324\n",
      "<Info> epoch: 13 loss: 0.21599344929556888\n",
      "<Info> epoch: 14 loss: 0.2158147543627299\n",
      "<Info> epoch: 15 loss: 0.21525648526203528\n",
      "<Info> epoch: 16 loss: 0.2150571803360995\n",
      "<Info> epoch: 17 loss: 0.21457312619058794\n",
      "<Info> epoch: 18 loss: 0.21429116538045592\n",
      "<Info> epoch: 19 loss: 0.21379702213970625\n",
      "<Info> epoch: 20 loss: 0.21350331466031425\n",
      "<Info> epoch: 21 loss: 0.21340242164631204\n",
      "<Info> epoch: 22 loss: 0.21309533032940214\n",
      "<Info> epoch: 23 loss: 0.21250814674693064\n",
      "<Info> epoch: 24 loss: 0.2126837346312277\n",
      "<Info> epoch: 25 loss: 0.21224333851462462\n",
      "<Info> epoch: 26 loss: 0.21187617582934504\n",
      "<Info> epoch: 27 loss: 0.2116210511493156\n",
      "<Info> epoch: 28 loss: 0.21132966955893231\n",
      "<Info> epoch: 29 loss: 0.2115155441347148\n",
      "<Info> epoch: 30 loss: 0.21059104052803243\n",
      "<Info> epoch: 31 loss: 0.21105106263804838\n",
      "<Info> epoch: 32 loss: 0.21047736243034418\n",
      "<Info> epoch: 33 loss: 0.21038202155562089\n",
      "<Info> epoch: 34 loss: 0.20988369785152633\n",
      "<Info> epoch: 35 loss: 0.21003508034964957\n",
      "<Info> epoch: 36 loss: 0.2100996224784001\n",
      "<Info> epoch: 37 loss: 0.21002984801262808\n",
      "<Info> epoch: 38 loss: 0.20947750237112175\n",
      "<Info> epoch: 39 loss: 0.20917684836290773\n",
      "<Info> epoch: 40 loss: 0.20904022433120636\n",
      "<Info> epoch: 41 loss: 0.20899152297870838\n",
      "<Info> epoch: 42 loss: 0.20906190236324085\n",
      "<Info> epoch: 43 loss: 0.20900698566226877\n",
      "<Info> epoch: 44 loss: 0.20830422535842066\n",
      "<Info> epoch: 45 loss: 0.2084519654506157\n",
      "<Info> epoch: 46 loss: 0.20840429746474204\n",
      "<Info> epoch: 47 loss: 0.20804207031684016\n",
      "<Info> epoch: 48 loss: 0.20759010461273636\n",
      "<Info> epoch: 49 loss: 0.2075150163944759\n",
      "<Info> epoch: 50 loss: 0.20748512431619218\n",
      "<Info> epoch: 51 loss: 0.20733717142576863\n",
      "<Info> epoch: 52 loss: 0.20734641539837548\n",
      "<Info> epoch: 53 loss: 0.20708603404015474\n",
      "<Info> epoch: 54 loss: 0.2066842265722038\n",
      "<Info> epoch: 55 loss: 0.20659546602038759\n",
      "<Info> epoch: 56 loss: 0.20627083673128166\n",
      "<Info> epoch: 57 loss: 0.20621728131840256\n",
      "<Info> epoch: 58 loss: 0.20601999072069963\n",
      "<Info> epoch: 59 loss: 0.20606072584226726\n",
      "<Info> epoch: 60 loss: 0.20597841559679217\n",
      "<Info> epoch: 61 loss: 0.20582533451772708\n",
      "<Info> epoch: 62 loss: 0.20531921747328663\n",
      "<Info> epoch: 63 loss: 0.2053914628242129\n",
      "<Info> epoch: 64 loss: 0.20527689247599315\n",
      "<Info> epoch: 65 loss: 0.20496387932514468\n",
      "<Info> epoch: 66 loss: 0.2049769651596058\n",
      "<Info> epoch: 67 loss: 0.20480569273538085\n",
      "<Info> epoch: 68 loss: 0.20448046289040572\n",
      "<Info> epoch: 69 loss: 0.20477593273371425\n",
      "<Info> epoch: 70 loss: 0.20498006383556922\n",
      "<Info> epoch: 71 loss: 0.20463148253256286\n",
      "<Info> epoch: 72 loss: 0.20499119505639662\n",
      "<Info> epoch: 73 loss: 0.20450650431914846\n",
      "<Info> epoch: 74 loss: 0.20412720514590915\n",
      "<Info> epoch: 75 loss: 0.20433755400347683\n",
      "<Info> epoch: 76 loss: 0.20382368528151926\n",
      "<Info> epoch: 77 loss: 0.20374677019250698\n",
      "<Info> epoch: 78 loss: 0.2036454337734846\n",
      "<Info> epoch: 79 loss: 0.20359786636219618\n",
      "<Info> epoch: 80 loss: 0.2036391957576123\n",
      "<Info> epoch: 81 loss: 0.20327744815388085\n",
      "<Info> epoch: 82 loss: 0.20307411877052214\n",
      "<Info> epoch: 83 loss: 0.20307551294666104\n",
      "<Info> epoch: 84 loss: 0.20295204538758188\n",
      "<Info> epoch: 85 loss: 0.202846519602931\n",
      "<Info> epoch: 86 loss: 0.202720367447534\n",
      "<Info> epoch: 87 loss: 0.2029322021005484\n",
      "<Info> epoch: 88 loss: 0.20282302931974666\n",
      "<Info> epoch: 89 loss: 0.20254621208027698\n",
      "<Info> epoch: 90 loss: 0.20238630149321044\n",
      "<Info> epoch: 91 loss: 0.20244452312626054\n",
      "<Info> epoch: 92 loss: 0.20211375210758914\n",
      "<Info> epoch: 93 loss: 0.2019557126320317\n",
      "<Info> epoch: 94 loss: 0.2020923886941222\n",
      "<Info> epoch: 95 loss: 0.20221621533501508\n",
      "<Info> epoch: 96 loss: 0.20144408059386126\n",
      "<Info> epoch: 97 loss: 0.20192599381795825\n",
      "<Info> epoch: 98 loss: 0.20160279832212466\n",
      "<Info> epoch: 99 loss: 0.2015150069684039\n",
      "<Info> epoch: 100 loss: 0.20148616609520534\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "print('<Info> Creating the architecture of the Neural Network')\n",
    "# Creating the architecture of the Neural Network\n",
    "nv = len(training_set[0])\n",
    "nh = 125\n",
    "batch_size = 128\n",
    "rbm = RBM(nv, nh, 10, 5e-3)\n",
    "\n",
    "# Training the RBM\n",
    "nb_epoch = 50\n",
    "prev_train_loss = 0\n",
    "s = float(torch.sum(training_set > 0))\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        batch = training_set[id_user:id_user+batch_size].to(device) / 5.0\n",
    "        batch_error = rbm.contrastive_divergence(batch)\n",
    "        train_loss += batch_error\n",
    "    train_loss = math.sqrt(train_loss / s)\n",
    "    print('<Info> epoch: '+str(epoch)+' loss: '+str(train_loss))\n",
    "    if abs(train_loss - prev_train_loss) < 1e-6:\n",
    "        break\n",
    "    prev_train_loss = train_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info> Testing the RBM\n",
      "<Info> Test loss: 0.1896444809380109\n",
      "<Info> Completed.\n"
     ]
    }
   ],
   "source": [
    "print('<Info> Testing the RBM')\n",
    "test_loss = 0\n",
    "s = float(torch.sum(test_set > 0))\n",
    "count = 0\n",
    "for id_user in range(nb_users):\n",
    "    # Use the training set to activate neurons  \n",
    "    v = test_set[id_user:id_user+1] / 5.0\n",
    "    h = rbm.sample_hidden(v)\n",
    "    vt = rbm.sample_visible(h)\n",
    "    v = torch.flatten(v)\n",
    "    vt = torch.flatten(vt)[v>0]\n",
    "    v = v[v>0]\n",
    "    test_loss += torch.sum((vt - v) ** 2)\n",
    "test_loss = math.sqrt(test_loss / s)        \n",
    "print('<Info> Test loss: '+ str(test_loss * 5))\n",
    "print('<Info> Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
