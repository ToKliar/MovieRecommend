{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\MovieRecommend\\mix_lfm.ipynb Cell 1\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MovieRecommend/mix_lfm.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Loading the cleaned datasets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MovieRecommend/mix_lfm.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m rnames \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmovie_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MovieRecommend/mix_lfm.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ratings_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_table(\u001b[39m\"\u001b[39;49m\u001b[39mml-1m/ratings.dat\u001b[39;49m\u001b[39m\"\u001b[39;49m, header \u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m::\u001b[39;49m\u001b[39m'\u001b[39;49m,names\u001b[39m=\u001b[39;49mrnames, engine\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mpython\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MovieRecommend/mix_lfm.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m uname \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39moccupation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MovieRecommend/mix_lfm.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m users_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_table(\u001b[39m\"\u001b[39m\u001b[39mml-1m/users.dat\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m::\u001b[39m\u001b[39m'\u001b[39m, header \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, names\u001b[39m=\u001b[39muname, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:689\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    685\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 689\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:468\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    467\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 468\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:1057\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, nrows\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1056\u001b[0m     nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m-> 1057\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   1059\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1060\u001b[0m         \u001b[39mif\u001b[39;00m col_dict:\n\u001b[0;32m   1061\u001b[0m             \u001b[39m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:2502\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2499\u001b[0m columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_dedup_names(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m   2500\u001b[0m columns, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_date_conversions(columns, data)\n\u001b[1;32m-> 2502\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_data(data)\n\u001b[0;32m   2503\u001b[0m index, columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_index(data, alldata, columns, indexnamerow)\n\u001b[0;32m   2505\u001b[0m \u001b[39mreturn\u001b[39;00m index, columns, data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:2582\u001b[0m, in \u001b[0;36mPythonParser._convert_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2579\u001b[0m     clean_na_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mna_values\n\u001b[0;32m   2580\u001b[0m     clean_na_fvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mna_fvalues\n\u001b[1;32m-> 2582\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_to_ndarrays(\n\u001b[0;32m   2583\u001b[0m     data,\n\u001b[0;32m   2584\u001b[0m     clean_na_values,\n\u001b[0;32m   2585\u001b[0m     clean_na_fvalues,\n\u001b[0;32m   2586\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   2587\u001b[0m     clean_conv,\n\u001b[0;32m   2588\u001b[0m     clean_dtypes,\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:1704\u001b[0m, in \u001b[0;36mParserBase._convert_to_ndarrays\u001b[1;34m(self, dct, na_values, na_fvalues, verbose, converters, dtypes)\u001b[0m\n\u001b[0;32m   1701\u001b[0m try_num_bool \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (cast_type \u001b[39mand\u001b[39;00m is_str_or_ea_dtype)\n\u001b[0;32m   1703\u001b[0m \u001b[39m# general type inference and conversion\u001b[39;00m\n\u001b[1;32m-> 1704\u001b[0m cvals, na_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_types(\n\u001b[0;32m   1705\u001b[0m     values, \u001b[39mset\u001b[39;49m(col_na_values) \u001b[39m|\u001b[39;49m col_na_fvalues, try_num_bool\n\u001b[0;32m   1706\u001b[0m )\n\u001b[0;32m   1708\u001b[0m \u001b[39m# type specified in dtype param or cast_type is an EA\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m cast_type \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   1710\u001b[0m     \u001b[39mnot\u001b[39;00m is_dtype_equal(cvals, cast_type)\n\u001b[0;32m   1711\u001b[0m     \u001b[39mor\u001b[39;00m is_extension_array_dtype(cast_type)\n\u001b[0;32m   1712\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\io\\parsers.py:1758\u001b[0m, in \u001b[0;36mParserBase._infer_types\u001b[1;34m(self, values, na_values, try_num_bool)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[39mif\u001b[39;00m try_num_bool \u001b[39mand\u001b[39;00m is_object_dtype(values\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   1756\u001b[0m     \u001b[39m# exclude e.g DatetimeIndex here\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1758\u001b[0m         result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_numeric(values, na_values, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1759\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m   1760\u001b[0m         \u001b[39m# e.g. encountering datetime string gets ValueError\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m         \u001b[39m#  TypeError can be raised in floatify\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m         result \u001b[39m=\u001b[39m values\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "mnames = ['movie_id', 'title', 'genre']\n",
    "movies_df = pd.read_table('ml-1m/movies.dat', names = mnames, sep = \"::\", engine = 'python', encoding='ISO-8859-1')\n",
    "# Loading the cleaned datasets\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings_df = pd.read_table(\"ml-1m/ratings.dat\", header =None, sep='::',names=rnames, engine= 'python')\n",
    "uname = ['user_id','gender','age','occupation','zip']\n",
    "users_df = pd.read_table(\"ml-1m/users.dat\", sep='::', header = None, names=uname, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the ratings dataset into the feature set (X) and target labels (y)\n",
    "X = ratings_df.drop(columns='rating')\n",
    "y = ratings_df[\"rating\"].values  # The movie ratings are the target variables we want to predict\n",
    "\n",
    "# Preparing train, validation and test datasets.\n",
    "# I have chosen a split ratio of 80%, 10%, 10%, because I want a somewhat large training set at the cost of a\n",
    "# smaller validation and test set. I do not think that a smaller validation (or test) dataset will negatively\n",
    "# impact the generalization ability of the chosen models, because I am only using rather simple ML models\n",
    "# with few hyperparamaters.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# Creating a complete training dataset with X_train and y_train\n",
    "train_df = X_train.copy()\n",
    "train_df[\"rating\"] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[\"rating\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(train_df)\n",
    "train_set = train_set.astype(\"int\")\n",
    "test_set = np.array(train_df)\n",
    "test_set = test_set.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3952\n"
     ]
    }
   ],
   "source": [
    "#take max users id in train and test data\n",
    "nb_users = int(max(max(train_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies =  int(max(max(train_set[:, 1]), max(test_set[:, 1])))\n",
    "print(nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    rating_data = np.zeros((nb_users, nb_movies))\n",
    "    timestamp_data = np.zeros((nb_users, nb_movies))\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        ##id of movies that is rated by current users\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        \n",
    "        ##rate of movies that is given by current user\n",
    "        id_ratings = data[:,3][data[:,0] == id_users]\n",
    "        id_timestamps = data[:, 2][data[:, 0] == id_users]\n",
    "        \n",
    "        #inialize ratings for all movies\n",
    "        #set 0 for movies that are not rated by current users\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        #movie id starts from 1, 1st movie will be 1st element in rating with index as 0\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        rating_data[id_users - 1] = ratings\n",
    "\n",
    "        timestamps = np.zeros(nb_movies)\n",
    "        timestamps[id_movies - 1] = id_timestamps\n",
    "        timestamp_data[id_users - 1] = timestamps\n",
    "    return rating_data, timestamp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate, train_timestamp = convert(train_set)\n",
    "test_rate, test_timestamp = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_df = pd.read_csv(\"./films.csv\")\n",
    "nb_genres = len(films_df.columns) - 2\n",
    "films_df = films_df.set_index('movie_id')\n",
    "films_df.drop(columns=[\"title\"], inplace=True)\n",
    "new_index = np.array(range(1, 3953))\n",
    "films_df = films_df.reindex(new_index, fill_value=0)\n",
    "films_matrix = np.array(films_df)\n",
    "films_matrix = films_matrix.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:02<00:00, 2636.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "users_prefer_matrix = np.zeros((nb_users, nb_genres))\n",
    "users_rating_matrix = np.zeros((nb_users, nb_genres))\n",
    "for i in tqdm(range(nb_users)):\n",
    "    films = films_matrix[train_rate[i] > 0]\n",
    "    ratings = train_rate[i][train_rate[i] > 0].reshape(films.shape[0])\n",
    "    timestamps = train_timestamp[i][train_rate[i] > 0]\n",
    "    \n",
    "    rate = 0.1\n",
    "    timestamps = (timestamps - np.min(timestamps)) / (np.max(timestamps) - np.min(timestamps)) * (-rate)\n",
    "    timestamps = np.exp(timestamps)\n",
    "    A = np.diag(np.ones(films.shape[0])) * timestamps * ratings \n",
    "    avg = np.ones((1, films.shape[0])) / films.shape[0]\n",
    "    P = np.dot(avg, np.dot(A, films)).reshape(-1)\n",
    "    users_prefer_matrix[i] = P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "F = 60\n",
    "lr = 0.015\n",
    "lam = 0.03\n",
    "epochs = 20\n",
    "N = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=k, random_state=101).fit(users_prefer_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "item_ids = set(ratings_df['movie_id'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "class Corpus:\n",
    "    @classmethod\n",
    "    def pre_process(cls, frame, index):\n",
    "        cls.frame = frame\n",
    "        cls.items_dict_path = 'lfm/lfm_items_{}.dict'.format(index)\n",
    "        cls.user_ids = set(cls.frame['user_id'].values)\n",
    "        cls.item_ids = item_ids\n",
    "        cls.items_dict = {user_id: cls._get_pos_neg_item(user_id) for user_id in list(cls.user_ids)}\n",
    "        cls.save(index)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_pos_neg_item(cls, user_id):\n",
    "        \"\"\"\n",
    "        Define the pos and neg item for user.\n",
    "        pos_item mean items that user have rating, and neg_item can be items\n",
    "        that user never see before.\n",
    "        Simple down sample method to solve unbalance sample.\n",
    "        \"\"\"\n",
    "        pos_item_ids = set(cls.frame[cls.frame['user_id'] == user_id]['movie_id'])\n",
    "        neg_item_ids = cls.item_ids ^ pos_item_ids\n",
    "        neg_item_ids = list(neg_item_ids)[:len(pos_item_ids)]\n",
    "        item_dict = {}\n",
    "        sub_frame = cls.frame[cls.frame['user_id'] == user_id]\n",
    "        for item in pos_item_ids: item_dict[item] = int(sub_frame[sub_frame['movie_id'] == item]['rating'])\n",
    "        for item in neg_item_ids: item_dict[item] = 0\n",
    "        return item_dict\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, index):\n",
    "        f = open('lfm/lfm_items_{}.dict'.format(index), 'wb')\n",
    "        pickle.dump(cls.items_dict, f)\n",
    "        f.close()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, index):\n",
    "        f = open('lfm/lfm_items_{}.dict'.format(index), 'rb')\n",
    "        items_dict = pickle.load(f)\n",
    "        f.close()\n",
    "        return items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 84344\n",
      "1 481080\n",
      "2 174028\n",
      "3 45380\n",
      "4 659208\n",
      "5 168660\n",
      "6 1269484\n",
      "7 272924\n",
      "8 45560\n"
     ]
    }
   ],
   "source": [
    "for index in range(9):\n",
    "    frame = train_df[kmeans.labels_[train_df['user_id']-1] == index]\n",
    "    print(index, frame.size)\n",
    "    Corpus.pre_process(frame, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_neighbors_N = np.zeros((nb_users, N))\n",
    "user_neighbors_index_N = np.zeros((nb_users, N))\n",
    "user_neighbors_sim_N = np.zeros((nb_users, N))\n",
    "user_neighbors_sim_sum = np.zeros((nb_users))\n",
    "cluster_prefix_matrix = []\n",
    "cluster_index = []\n",
    "for i in range(k):\n",
    "    cluster_prefix_matrix.append(users_prefer_matrix[kmeans.labels_ == i])\n",
    "    cluster_index.append(np.argwhere(kmeans.labels_ == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [00:00, 37865.33it/s]\n",
      "945it [00:00, 32578.39it/s]\n",
      "517it [00:00, 36919.93it/s]\n",
      "119it [00:00, 39656.93it/s]\n",
      "996it [00:00, 34337.44it/s]\n",
      "491it [00:00, 37762.96it/s]\n",
      "1680it [00:00, 28958.81it/s]\n",
      "829it [00:00, 30696.71it/s]\n",
      "160it [00:00, 39993.36it/s]\n"
     ]
    }
   ],
   "source": [
    "sum_user = 0\n",
    "for i in range(k):\n",
    "    group_index = cluster_index[i]\n",
    "    sum_user += len(group_index)\n",
    "    group_prefer_matrix = cluster_prefix_matrix[i]\n",
    "    norm = np.linalg.norm(group_prefer_matrix, axis=1, keepdims=True)\n",
    "    group_sim_matrix = (np.dot(group_prefer_matrix, group_prefer_matrix.T)) / (norm * norm.T)\n",
    "    for i in range(len(group_index)):\n",
    "        group_sim_matrix[i, i] = 0\n",
    "\n",
    "    for index, user in tqdm(enumerate(group_index)):\n",
    "        neighbors = np.argpartition(group_sim_matrix[index], -N)[-N:]\n",
    "        user_neighbors_index_N[user] = group_index[neighbors].reshape(1, -1).astype(int)\n",
    "        user_neighbors_N[user] = (group_index[neighbors].reshape(1, -1) + 1).astype(np.int64)\n",
    "        user_neighbors_sim_N[user] = group_sim_matrix[index, neighbors]\n",
    "        user_neighbors_sim_sum[user] = np.sum(user_neighbors_sim_N[user])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM:\n",
    "    def __init__(self, F, lr, lam, epochs, frame, index):\n",
    "        self.class_count = F\n",
    "        self.iter_count = epochs\n",
    "        self.lr = lr\n",
    "        self.lam = lam\n",
    "        self._init_model(frame, index)\n",
    "\n",
    "    def _init_model(self, frame, index):\n",
    "        \"\"\"\n",
    "        Get corpus and initialize model params.\n",
    "        \"\"\"\n",
    "        self.frame = frame\n",
    "        self.user_ids = set(self.frame['user_id'].values)\n",
    "        self.item_ids = item_ids\n",
    "        self.items_dict = Corpus.load(index)\n",
    "        self.index = index\n",
    "\n",
    "        array_p = np.random.randn(len(self.user_ids), self.class_count)\n",
    "        array_q = np.random.randn(len(self.item_ids), self.class_count)\n",
    "        self.p = pd.DataFrame(array_p, columns=range(0, self.class_count), index=list(self.user_ids))\n",
    "        self.q = pd.DataFrame(array_q, columns=range(0, self.class_count), index=list(self.item_ids))\n",
    "        \n",
    "    def _predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        Calculate interest between user_id and item_id.\n",
    "        p is the look-up-table for user's interest of each class.\n",
    "        q means the probability of each item being classified as each class.\n",
    "        \"\"\"\n",
    "        p = np.mat(self.p.loc[user_id].values)\n",
    "        q = np.mat(self.q.loc[item_id].values).T\n",
    "        r = (p * q).sum()\n",
    "        \n",
    "        # p = self.p.loc[user_neighbors_N[user_id - 1]].values # N * F\n",
    "        # rate = train_rate[user_neighbors_index_N[user_id - 1].astype(int), item_id - 1].reshape(-1, 1)\n",
    "        # neigh = (np.dot(p, q) - rate) * user_neighbors_sim_N[user_id - 1] / user_neighbors_sim_sum[user_id - 1]\n",
    "        # print(neigh)\n",
    "        return r\n",
    "\n",
    "    def _loss(self, user_id, item_id, y, step):\n",
    "        \"\"\"\n",
    "        Loss Function define as MSE, the code write here not that formula you think.\n",
    "        \"\"\"\n",
    "        e = y - self._predict(user_id, item_id)\n",
    "        return e\n",
    "\n",
    "    def _optimize(self, user_id, item_id, e):\n",
    "        \"\"\"\n",
    "        Use SGD as optimizer, with L2 p, q square regular.\n",
    "        e.g: E = 1/2 * (y - predict)^2, predict = matrix_p * matrix_q\n",
    "             derivation(E, p) = -matrix_q*(y - predict), derivation(E, q) = -matrix_p*(y - predict),\n",
    "             derivation（l2_square，p) = lam * p, derivation（l2_square, q) = lam * q\n",
    "             delta_p = lr * (derivation(E, p) + derivation（l2_square，p))\n",
    "             delta_q = lr * (derivation(E, q) + derivation（l2_square, q))\n",
    "        \"\"\"\n",
    "        gradient_p = -e * self.q.loc[item_id].values\n",
    "        l2_p = self.lam * self.p.loc[user_id].values\n",
    "        delta_p = self.lr * (gradient_p + l2_p)\n",
    "\n",
    "        gradient_q = -e * self.p.loc[user_id].values\n",
    "        l2_q = self.lam * self.q.loc[item_id].values\n",
    "        delta_q = self.lr * (gradient_q + l2_q)\n",
    "\n",
    "        self.p.loc[user_id] -= delta_p\n",
    "        self.q.loc[item_id] -= delta_q\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        for step in tqdm(range(0, self.iter_count)):\n",
    "            for user_id, item_dict in self.items_dict.items():\n",
    "                item_ids = list(item_dict.keys())\n",
    "                random.shuffle(item_ids)\n",
    "                for item_id in item_ids:\n",
    "                    e = self._loss(user_id, item_id, item_dict[item_id] / 5, step)\n",
    "                    self._optimize(user_id, item_id, e)\n",
    "            self.lr *= 0.9\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, user_id, top_n=10):\n",
    "        \"\"\"\n",
    "        Calculate all item user have not meet before and return the top n interest items.\n",
    "        \"\"\"\n",
    "        user_item_ids = set(self.frame[self.frame['user_id'] == user_id]['movie_id'])\n",
    "        other_item_ids = self.item_ids ^ user_item_ids\n",
    "        interest_list = [self._predict(user_id, item_id) for item_id in other_item_ids]\n",
    "        candidates = sorted(zip(list(other_item_ids), interest_list), key=lambda x: x[1], reverse=True)\n",
    "        return candidates[:top_n]\n",
    "    \n",
    "    def predictRate(self, user_id, item_id):\n",
    "        return self._predict(user_id, item_id)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Save model params.\n",
    "        \"\"\"\n",
    "        p = self.p.reset_index(drop=False)\n",
    "        q = self.q.reset_index(drop=False)\n",
    "\n",
    "        p.to_csv(\"lfm/lfm_p_{}.csv\".format(self.index), index=False)\n",
    "        q.to_csv(\"lfm/lfm_q_{}.csv\".format(self.index), index=False)\n",
    "        # f = open('lfm/lfm_{}.model'.format(self.index), 'wb')\n",
    "        # pickle.dump((self.p, self.q), f)\n",
    "        # f.close()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load model params.\n",
    "        \"\"\"\n",
    "        self.p = pd.read_csv(\"lfm/lfm_p_{}.csv\".format(self.index))\n",
    "        self.p.set_index('index',drop=True, append=False, inplace=True, verify_integrity=False)\n",
    "        self.q = pd.read_csv(\"lfm/lfm_q_{}.csv\".format(self.index))\n",
    "        self.q.set_index('index',drop=True, append=False, inplace=True, verify_integrity=False)\n",
    "\n",
    "        # f = open('lfm/lfm_{}.model'.format(self.index), 'rb')\n",
    "        # self.p, self.q = pickle.load(f)\n",
    "        # print(self.p, self.q)\n",
    "        # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [10:57<1:02:05, 219.15s/it]"
     ]
    }
   ],
   "source": [
    "for index in range(1, k):\n",
    "    frame = train_df[kmeans.labels_[train_df['user_id']-1] == index]\n",
    "    lfm = LFM(F, lr, lam, epochs, frame, index)\n",
    "    lfm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.22578462168399\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "test_loss = 0\n",
    "s = float(np.sum(test_set > 0))\n",
    "for i in range(k):\n",
    "    frame = train_df[kmeans.labels_[train_df['user_id']-1] == i]\n",
    "    lfm = LFM(F, lr, lam, epochs, frame, i)\n",
    "    lfm.load()\n",
    "    for user in cluster_index[i]:\n",
    "        items = test_rate[user][0]\n",
    "        for index, item in enumerate(items):\n",
    "            if item > 0:\n",
    "                pred = lfm.predictRate(user[0] + 1, index + 1)\n",
    "                test_loss += (pred * 5 - item) ** 2\n",
    "\n",
    "test_loss = math.sqrt(test_loss / s)     \n",
    "print(test_loss)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
